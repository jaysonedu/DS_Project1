{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH1Yu8XoOazy"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_0MUSeROfwe"
      },
      "source": [
        "In this notebook, the goal of feature engineering is to transform raw variables into features that are \\\n",
        "(1) comparable across ZIP codes \\\n",
        "(2) informative for modeling housing prices \\\n",
        "(3) able to capture time dynamics like momentum, trend, and seasonality. \\\n",
        "\\\n",
        "We intentionally create:\n",
        "\n",
        "Rate and share features (e.g., crime per 1,000 residents, education shares) because raw counts are heavily driven by population size.\n",
        "\n",
        "Time-series features (lags, rolling averages, year-over-year changes) because housing markets and neighborhood factors evolve over time and tend to be autocorrelated.\n",
        "\n",
        "Seasonality encodings because housing-related behavior often changes by month, and months are cyclical (December and January are adjacent even though 12 and 1 are numerically far apart).\n",
        "\n",
        "The output of this notebook is a modeling-ready dataset saved to `data/processed/model_data_fe.parquet` and `model_data_fe.csv`. It uses the preprocessed panel from `02_Data_Preprocessing` (`data/processed/model_data.parquet` or `model_data.csv`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54FDbpOWO9HM"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Project root (run from notebooks/ or project root)\n",
        "ROOT = Path.cwd()\n",
        "while ROOT != ROOT.parent and not (ROOT / \"pyproject.toml\").exists():\n",
        "    ROOT = ROOT.parent\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PTQCGt7uPGd3"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'final_clean_dataset.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m IN_PATH = \u001b[33m\"\u001b[39m\u001b[33mfinal_clean_dataset.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m OUT_PATH = \u001b[33m\"\u001b[39m\u001b[33mfinal_fe_dataset.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIN_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_div\u001b[39m(a, b):\n\u001b[32m     10\u001b[39m     b = b.replace({\u001b[32m0\u001b[39m: np.nan})\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\jason\\Columbia_stuff\\2025-2026\\DS\\project1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\jason\\Columbia_stuff\\2025-2026\\DS\\project1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\jason\\Columbia_stuff\\2025-2026\\DS\\project1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\jason\\Columbia_stuff\\2025-2026\\DS\\project1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\jason\\Columbia_stuff\\2025-2026\\DS\\project1\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'final_clean_dataset.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load preprocessed output from 02_Data_Preprocessing (data/processed/model_data.*)\n",
        "processed_dir = ROOT / \"data\" / \"processed\"\n",
        "in_parquet = processed_dir / \"model_data.parquet\"\n",
        "in_csv = processed_dir / \"model_data.csv\"\n",
        "if in_parquet.exists():\n",
        "    df = pd.read_parquet(in_parquet)\n",
        "elif in_csv.exists():\n",
        "    df = pd.read_csv(in_csv)\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No model_data.parquet or model_data.csv found in {processed_dir}. \"\n",
        "        \"Run 02_Data_Preprocessing.ipynb through the save cell first.\"\n",
        "    )\n",
        "\n",
        "# Output paths for feature-engineered dataset\n",
        "out_dir = ROOT / \"data\" / \"processed\"\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "OUT_PATH_CSV = out_dir / \"model_data_fe.csv\"\n",
        "OUT_PATH_PARQUET = out_dir / \"model_data_fe.parquet\"\n",
        "\n",
        "def safe_div(a, b):\n",
        "    b = b.replace({0: np.nan})\n",
        "    return a / b\n",
        "\n",
        "# Parse month into datetime (preprocessing output is YYYY-MM string)\n",
        "month_as_str = df[\"month\"].astype(str)\n",
        "if month_as_str.str.len().eq(7).all():  # YYYY-MM\n",
        "    df[\"month\"] = pd.to_datetime(month_as_str + \"-01\", errors=\"coerce\")\n",
        "else:\n",
        "    df[\"month\"] = pd.to_datetime(month_as_str, errors=\"coerce\")\n",
        "\n",
        "g = df.groupby(\"zip\", sort=False)\n",
        "print(f\"Loaded: {in_parquet if in_parquet.exists() else in_csv} -> shape {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLBx79TVPyN7"
      },
      "source": [
        "### Cross-Sectional Feature Engineering: Rates, Shares, and Log Transformations\n",
        "\n",
        "The following features are **cross-sectional transformations**, meaning they describe the structural characteristics of a neighborhood at a given point in time rather than its temporal dynamics.\n",
        "\n",
        "- **Crime per 1,000 residents** converts raw crime counts into an intensity measure. This ensures comparability across ZIP codes with different population sizes, preventing larger areas from appearing riskier simply due to scale.\n",
        "\n",
        "- **Education shares** transform degree counts into proportions of the total population. Proportions are generally more interpretable and stable than raw counts, allowing the model to learn structural differences in educational attainment.\n",
        "\n",
        "- **Log-transformed median income** reduces right-skewness in income distributions. Log transformation stabilizes variance, reduces the influence of extreme values, and often improves the performance of linear models.\n",
        "\n",
        "These transformations improve interpretability, reduce scale bias, and provide a more meaningful representation of neighborhood characteristics for downstream modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bywjIBA6PP4A",
        "outputId": "80ae97be-7030-4c3b-f381-47fba163c68d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Crime intensity (per 1,000 residents)\n",
        "df[\"crime_per_1000\"] = safe_div(df[\"crime_count\"], df[\"population\"]) * 1000\n",
        "\n",
        "# Education shares (normalize by population)\n",
        "df[\"higher_ed_share\"] = safe_div(df[\"higher_ed_count\"], df[\"population\"])\n",
        "df[\"edu_bachelors_share\"] = safe_div(df[\"edu_bachelors\"], df[\"population\"])\n",
        "df[\"edu_masters_share\"] = safe_div(df[\"edu_masters\"], df[\"population\"])\n",
        "df[\"edu_professional_share\"] = safe_div(df[\"edu_professional\"], df[\"population\"])\n",
        "df[\"edu_doctorate_share\"] = safe_div(df[\"edu_doctorate\"], df[\"population\"])\n",
        "\n",
        "# Graduate-or-higher share (Masters + Professional + Doctorate)\n",
        "df[\"edu_gradplus_share\"] = safe_div(\n",
        "    df[\"edu_masters\"] + df[\"edu_professional\"] + df[\"edu_doctorate\"],\n",
        "    df[\"population\"]\n",
        ")\n",
        "\n",
        "# Log-transform income (reduces skew; use max(0) to avoid log of negative/NaN)\n",
        "df[\"log_median_income\"] = np.log1p(np.maximum(df[\"median_income\"], 0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1eix_1rQAfe"
      },
      "source": [
        "## Time-Series Features\n",
        "\n",
        "Housing markets exhibit strong temporal dependence. Current prices are influenced by recent history, and neighborhood conditions may evolve gradually rather than abruptly. To capture these dynamics, we construct several types of time-based features.\n",
        "\n",
        "**Lag features** (e.g., 1-, 3-, 6-, and 12-month lags) capture persistence and seasonal effects. These allow the model to learn autoregressive behavior, such as housing price momentum.\n",
        "\n",
        "**Percentage change features** (month-over-month and year-over-year) represent growth dynamics rather than static levels. These features help capture acceleration, deceleration, or structural shifts in trends.\n",
        "\n",
        "**Rolling means** summarize recent history and reduce short-term noise. They provide a smoothed representation of local conditions and often improve generalization.\n",
        "\n",
        "**Rolling standard deviations** measure volatility, capturing the stability or instability of housing prices or crime rates. Volatility can signal transitional market phases or changing neighborhood conditions.\n",
        "\n",
        "To avoid data leakage, rolling statistics are computed using only past information (via shifting before rolling).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "20M9EBE6QIxh"
      },
      "outputs": [],
      "source": [
        "def add_ts_features(col, lags=(1, 3, 6, 12), rolls=(3, 6, 12), add_std=(6, 12), prefix=None):\n",
        "    if prefix is None:\n",
        "        prefix = col\n",
        "\n",
        "    # Lags\n",
        "    for k in lags:\n",
        "        df[f\"{prefix}_lag{k}\"] = g[col].shift(k)\n",
        "\n",
        "    # Month-over-month and year-over-year changes\n",
        "    df[f\"{prefix}_pct_change_1m\"] = g[col].pct_change(1)\n",
        "    df[f\"{prefix}_pct_change_12m\"] = g[col].pct_change(12)\n",
        "\n",
        "    # Rolling mean and std computed using only past information (shifted by 1)\n",
        "    for w in rolls:\n",
        "        df[f\"{prefix}_roll_mean_{w}\"] = (\n",
        "            g[col].shift(1).rolling(w).mean().reset_index(level=0, drop=True)\n",
        "        )\n",
        "    for w in add_std:\n",
        "        df[f\"{prefix}_roll_std_{w}\"] = (\n",
        "            g[col].shift(1).rolling(w).std().reset_index(level=0, drop=True)\n",
        "        )\n",
        "\n",
        "# Price dynamics (ZHVI)\n",
        "add_ts_features(\"zhvi\", prefix=\"zhvi\")\n",
        "\n",
        "# Crime dynamics (use rate, not raw count)\n",
        "add_ts_features(\"crime_per_1000\", prefix=\"crime_per_1000\", add_std=(6, 12))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KpKYRecQMtT"
      },
      "source": [
        "## Seasonality Encoding\n",
        "\n",
        "Month-of-year effects are cyclical. Using the numeric month (1â€“12) directly imposes an artificial ordering where December and January appear far apart. To preserve the cyclical structure of time, we encode month using sine and cosine transformations.\n",
        "\n",
        "This approach allows the model to learn seasonal housing patterns (e.g., spring buying cycles) without introducing discontinuities in the feature space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AhCUI_lLQM9M"
      },
      "outputs": [],
      "source": [
        "df[\"year\"] = df[\"month\"].dt.year\n",
        "df[\"month_num\"] = df[\"month\"].dt.month\n",
        "\n",
        "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month_num\"] / 12)\n",
        "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month_num\"] / 12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03YZ3VwhQUGn"
      },
      "source": [
        "## cleanup + export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Zg786TQXuo",
        "outputId": "f081be68-d010-4350-aed0-726e1e825585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: final_fe_dataset.csv\n",
            "Shape: (940922, 52)\n"
          ]
        }
      ],
      "source": [
        "df.to_csv(OUT_PATH_CSV, index=False)\n",
        "df.to_parquet(OUT_PATH_PARQUET, index=False)\n",
        "print(\"Saved:\", OUT_PATH_CSV)\n",
        "print(\"Saved:\", OUT_PATH_PARQUET)\n",
        "print(\"Shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3kNQLNRQmTU"
      },
      "source": [
        "## Overall Impact on Modeling\n",
        "\n",
        "Collectively, these engineered features allow the model to learn:\n",
        "\n",
        "- Structural neighborhood differences (rates and shares)\n",
        "- Temporal persistence and momentum (lags)\n",
        "- Trend dynamics (percentage changes)\n",
        "- Stability and risk (rolling volatility)\n",
        "- Seasonal patterns (cyclical encoding)\n",
        "\n",
        "By transforming raw observations into economically meaningful signals, feature engineering enhances predictive performance, interpretability, and robustness."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
